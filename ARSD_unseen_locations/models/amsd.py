import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import OrderedDict
from models import register

class FactorizedConv(nn.Module):
    r"""
    Factorized $n \times n$ convolution into $n \times 1$ and $1 \times n$ convolution.
    Args:
        kernel_size (int): The larger size (n) of the convolution.
        in_channels (int): Number of input channels.
        out_channels (int): Number of output channels.
    """
    def __init__(self, kernel_size, in_channels, inner_channels, out_channels):
        super(FactorizedConv, self).__init__()
        h_kernel, h_padding = (1, kernel_size), (0, kernel_size // 2)
        v_kernel, v_padding = (kernel_size, 1), (kernel_size // 2, 0)

        self.horizontal_conv = nn.Conv2d(in_channels, inner_channels, kernel_size=h_kernel, padding=h_padding)
        self.vertical_conv = nn.Conv2d(in_channels, inner_channels, kernel_size=v_kernel, padding=v_padding)

        self.output_conv = nn.Sequential(OrderedDict([
            ('act in', nn.PReLU(2 * inner_channels)),
            ('conv', nn.Conv2d(2 * inner_channels, out_channels, kernel_size=(1, 1))),
            ('act out', nn.PReLU(out_channels))
        ]))

    def forward(self, x):
        x = torch.cat([self.vertical_conv(x), self.horizontal_conv(x)], dim=1)
        return self.output_conv(x)


class DenseBlock(nn.Module):
    r"""
    Dense Convolutional Block and attention block
    Args:
        in_channels (int): Number of block's input channels.
        inner_channels (int): Number of channels generated by the inner convolutions.
        out_channels (int): Number of block's output channels.
        num_dense_convs (int): Number of dense convolutions.
        scale_factor (bool): Use the scale factor as additional information.
        attention (bool): Use the attention block.
    """
    def __init__(self, in_channels, inner_channels, out_channels, num_dense_convs=2, scale_factor=False, attention=False):
        super(DenseBlock, self).__init__()
        self.out_channels = out_channels
        self.num_dense_convs = num_dense_convs
        self.use_scale_factor = scale_factor
        self.use_attention = attention

        # Base (input) convolution
        self.conv_00=nn.Sequential(OrderedDict([
            ('conv', nn.Conv2d(in_channels, inner_channels, kernel_size=(1, 1))),
            ('act', nn.PReLU(inner_channels))
        ]))


        self.conv_0 = nn.Sequential(OrderedDict([
            ('conv', nn.Conv2d(inner_channels, inner_channels, kernel_size=(3, 3), padding=(1, 1))),
            ('act', nn.PReLU(inner_channels))
        ]))

        list_dense_convs = []
        for i in range(1, 1 + num_dense_convs):
            list_dense_convs.append(
                nn.Sequential(OrderedDict([
                    ('conv', nn.Conv2d(inner_channels + i * inner_channels, inner_channels, kernel_size=(3, 3), padding=(1, 1))),
                    ('act', nn.PReLU(inner_channels))
                ]))
            )

        self.dense_convs = nn.ModuleList(list_dense_convs)

        # Last convolution
        if scale_factor and not attention:
            in_channels_last_conv = inner_channels + (num_dense_convs + 1) * inner_channels + 1
        else:
            in_channels_last_conv = inner_channels + (num_dense_convs + 1) * inner_channels
        self.conv_1 = nn.Sequential(OrderedDict([
            ('conv', nn.Conv2d(in_channels_last_conv, out_channels, kernel_size=(1, 1))),
            ('act', nn.PReLU(out_channels))
        ]))

        if self.use_attention:
            linear_in_features = 1 + 16 * inner_channels if scale_factor else 16 * inner_channels
            self.pooling = nn.AdaptiveAvgPool2d((4, 4))
            self.attention_block = nn.Sequential(OrderedDict([
                ('linear 0', nn.Linear(linear_in_features, inner_channels)),
                ('act 0', nn.PReLU(inner_channels)),
                ('linear 1', nn.Linear(inner_channels, out_channels)),
                ('act 1', nn.Sigmoid())
            ]))

    def forward(self, x, scale_factor=None):
        # Dense convolutions
        x=self.conv_00(x)
        out = torch.cat([x, self.conv_0(x)], dim=1)
        for i in range(self.num_dense_convs):
            out = torch.cat([out, self.dense_convs[i](out)], dim=1)

        if self.use_attention:
            out = self.conv_1(out)
            att = self.pooling(out)
            b = att.size()[0]
            att = att.view(b, -1)
            if scale_factor is not None and self.use_scale_factor:      # Use attention and scale factor information.
                scale_factor = scale_factor.view(b, 1)
                att = torch.cat([att, scale_factor], dim=1)

            att = self.attention_block(att)
            att = att.view(-1, self.out_channels, 1, 1)

            out = out * att + x
        else:
            if scale_factor is not None and self.use_scale_factor:      # Use scale factor in a feature map.
                b, _, h, w = out.size()
                scale_factor = scale_factor.view(1, 1, 1, 1).repeat(b, 1, h, w)
                out = torch.cat([out, scale_factor], dim=1)

            out = self.conv_1(out) + x

        return out


class AMSD(nn.Module):

    def __init__(self):
        super().__init__()
        base_filters = 64
        num_dense_convs = 5
        self.use_scale_factor = False
        self.use_attention = True
        self.use_grl = True
        self.out_dim=base_filters
        #第一级特征提取
        self.input_conv = nn.Sequential(OrderedDict([
            ('conv 0', nn.Conv2d(in_channels=1, out_channels=base_filters, kernel_size=(3, 3), padding=(1, 1))),
            ('act 0', nn.PReLU(base_filters)),
            ('conv 1', nn.Conv2d(in_channels=base_filters, out_channels=base_filters, kernel_size=(3, 3), padding=(1, 1))),
            ('act 1', nn.PReLU(base_filters))
        ]))

        #多分辨率特征融合
        self.mul_fuseX2=nn.ModuleList([
            DenseBlock(base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
            DenseBlock(3*base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
            DenseBlock(3*base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
        ])
        self.mul_fuse=nn.ModuleList([
            DenseBlock(base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
            DenseBlock(3*base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
            DenseBlock(3*base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
        ])
        self.mul_fuseD2=nn.ModuleList([
            DenseBlock(base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
            DenseBlock(3*base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
            DenseBlock(3*base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
        ])

        #第二级特征提取
        # self.block_0 = nn.ModuleList([
        #     DenseBlock(base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
        #     DenseBlock(base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
        #     DenseBlock(base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
        #     DenseBlock(base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
        #     FactorizedConv(1, 4 * base_filters, 4 * base_filters, base_filters)
        # ])
        #
        # self.block_1 = nn.ModuleList([
        #     DenseBlock(base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
        #     DenseBlock(base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
        #     DenseBlock(base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
        #     DenseBlock(base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
        #     FactorizedConv(3, 4 * base_filters, 4 * base_filters, base_filters)
        # ])
        #
        # self.block_2 = nn.ModuleList([
        #     DenseBlock(base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
        #     DenseBlock(base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
        #     DenseBlock(base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
        #     DenseBlock(base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
        #     FactorizedConv(3, 4 * base_filters, 4 * base_filters, base_filters)
        # ])

        self.block_3 = nn.ModuleList([
            DenseBlock(base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
            DenseBlock(base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
            DenseBlock(base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
            DenseBlock(base_filters, base_filters, base_filters, num_dense_convs, self.use_scale_factor, self.use_attention),
            FactorizedConv(3, 4 * base_filters, 4 * base_filters, base_filters)
        ])

    def up_x2(self,data):
        h,w=data.size()[-2:]
        return F.interpolate(data, size=(h*2,w*2), mode='bicubic', align_corners=False)
    def up_x4(self,data):
        h, w = data.size()[-2:]
        return F.interpolate(data, size=(h*4,w*4), mode='bicubic', align_corners=False)
    def down_d2(self,data):
        h, w = data.size()[-2:]
        return F.interpolate(data, size=(h//2,w//2), mode='bicubic', align_corners=False)
    def down_d4(self,data):
        h, w = data.size()[-2:]
        return F.interpolate(data, size=(h//4,w//4), mode='bicubic', align_corners=False)

    def forward(self, inputs,scale=None):

        if self.use_scale_factor:
            scale = torch.tensor(scale).to(inputs.device)
        else:
            scale = None
        # Low resolution
        outputs = self.input_conv(inputs)

        res=outputs
        #多分辨率特征融合
        x2 = self.mul_fuseX2[0](self.up_x2(outputs),scale)
        x = self.mul_fuse[0](outputs,scale)
        d2 = self.mul_fuseD2[0](self.down_d2(outputs),scale)
        for i in range(1,3):
            x2=self.mul_fuseX2[i](torch.cat([x2,self.up_x2(x),self.up_x4(d2)], dim=1),scale)
            x=self.mul_fuse[i](torch.cat([self.down_d2(x2),x,self.up_x2(d2)], dim=1),scale)
            d2=self.mul_fuseD2[i](torch.cat([self.down_d4(x2),self.down_d2(x),d2], dim=1),scale)
        outputs=self.down_d2(x2)+x+self.up_x2(d2)

        outputs+=res
        to_add = outputs

        #第二级特征提取
        # Block 0
        # block = []
        # for i in range(len(self.block_0) - 1):
        #     outputs = self.block_0[i](outputs, scale)
        #     block.append(outputs)
        # outputs = self.block_0[-1](torch.cat(block, dim=1)) + to_add
        # # to_add = outputs
        #
        # # Block 1
        # block = []
        # for i in range(len(self.block_1) - 1):
        #     outputs = self.block_1[i](outputs, scale)
        #     block.append(outputs)
        # outputs = self.block_1[-1](torch.cat(block, dim=1)) + to_add
        # to_add = outputs
        #
        # # Block 2
        # block = []
        # for i in range(len(self.block_2) - 1):
        #     outputs = self.block_2[i](outputs, scale)
        #     block.append(outputs)
        # outputs = self.block_2[-1](torch.cat(block, dim=1)) + to_add
        # to_add = outputs

        # Block 3
        block = []
        for i in range(len(self.block_3) - 1):
            outputs = self.block_3[i](outputs, scale)
            block.append(outputs)
        outputs = self.block_3[-1](torch.cat(block, dim=1)) + to_add

        # Final section
        outputs+=to_add

        return outputs
@register('amsd')
def make_amsd(no_upsampling=True):
    return AMSD()


if __name__=='__main__':
    # args = Args()
    net = AMSD()
    # net.set_scale(2.0, 2.0)

    x = torch.rand(1, 192, 227, 315)
    y=net(x)
    print("Output shape:", y.shape)

    total_params = sum(p.numel() for p in net.parameters())
    # print(net)
    print("Total number of parameters:", total_params)
    print("Output shape:", y.shape)
